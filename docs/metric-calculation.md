# Metric calculation
This section describes how to calculate metrics for the datasets generated by the [OT Platform ETL](https://github.com/opentargets/platform-etl-backend).


## Configure the run
Make changes to `config/config.yaml`.

If you are running in pre-ETL mode, set:
* `is_pre_etl_run` = `true`.
* `run_id` = `XX.YY-pre`, substituting the upcoming release, for example 23.12.
* `etl_run_id` = `null`.

If you are running in post-ETL mode, set:
* `is_pre_etl_run` = `false` (this is the default).
* `run_id` = `XX.YY.YYYYMMDD`, substituting XX.YY with the upcoming release, and YYYYMMDD with the timestamp for the date on which the ETL was run. This is essential to be able to compare multiple ETL runs throughout the release preparation process.
* `etl_run_id` = `XX.YY`, substituing XX.YY with the upcoming release. Consequent ETL runs are stored in the same bucket, overwriting its contents, so this path is the same for all ETL runs for the release.


## Submit job to GCP
The below commands should not need to be modified. They will create a Google Dataproc cluster and submit the job which will calculate the metrics and upload them to the output bucket in Google Storage.

```bash
# Cluster parameters.
export CLUSTER_NAME=ot-release-metrics
export CLUSTER_REGION=europe-west1
export CLUSTER_INIT_ACTIONS=gs://otar000-evidence_input/release-metrics/initialise_cluster.sh

# Create Dataproc cluster.
gsutil cp src/initialise_cluster.sh ${CLUSTER_INIT_ACTIONS}
gcloud dataproc clusters create ${CLUSTER_NAME} \
    --image-version=2.1 \
    --single-node \
    --region=${CLUSTER_REGION} \
    --metadata 'PIP_PACKAGES=hydra-core==1.2.0 gcsfs==2022.7.1' \
    --initialization-actions ${CLUSTER_INIT_ACTIONS} \
    --master-machine-type=n1-standard-64 \
    --master-boot-disk-size=100 \
    --max-idle=10m \
    --project open-targets-eu-dev

# Package code and submit a PySpark job to the Dataproc cluster.
zip -r code_bundle.zip . -x "src/metric_calculation/metrics.py" "env/*" "src/assets/*" ".git/*" "outputs/*" "docs/*" "code_bundle.zip"
gcloud dataproc jobs submit pyspark \
  src/metric_calculation/metrics.py \
  --cluster=${CLUSTER_NAME} \
  --region=${CLUSTER_REGION} \
  --py-files code_bundle.zip \
  --files='config/config.yaml' \
  --project open-targets-eu-dev
```


## Updating the Streamlit app
If the Streamlit app was already deployed at the time when the above code was run, it will not be able to automatically pick up the new metrics. Reboot it manually to reflect the new changes. See instructions in the [app documentation](metric-visualisation.md#rebooting-the-app).
